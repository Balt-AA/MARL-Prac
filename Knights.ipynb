{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c4948c-b06f-40ec-ac17-b60cb69ad22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51a3005-5517-42e5-adaf-c3a32d3696ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on knights_archers_zombies_v10.\n",
      "Using cpu device\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 981   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 862        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 152        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00868552 |\n",
      "|    clip_fraction        | 0.0759     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.78      |\n",
      "|    explained_variance   | -0.633     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00749    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0049    |\n",
      "|    value_loss           | 0.03       |\n",
      "----------------------------------------\n",
      "Model has been saved.\n",
      "Finished training on knights_archers_zombies_v10.\n",
      "\n",
      "Starting evaluation on knights_archers_zombies_v10 (num_games=10, render_mode=None)\n",
      "Avg reward: 3.5\n",
      "Avg reward per agent, per game:  {'archer_0': 0.6, 'archer_1': 0.8, 'knight_0': 0.0, 'knight_1': 0.0}\n",
      "Full rewards:  {'archer_0': 6, 'archer_1': 8, 'knight_0': 0, 'knight_1': 0}\n"
     ]
    }
   ],
   "source": [
    "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
    "    # Train a single model to play as each agent in an AEC environment\n",
    "    env = env_fn.parallel_env(**env_kwargs)\n",
    "\n",
    "    # Add black death wrapper so the number of agents stays constant\n",
    "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "    env = ss.black_death_v3(env)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Use a CNN policy if the observation space is visual\n",
    "    model = PPO(\n",
    "        CnnPolicy if visual_observation else MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
    "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample()\n",
    "                else:\n",
    "                    act = model.predict(obs, deterministic=True)[0]\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    avg_reward_per_agent = {\n",
    "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
    "    }\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
    "    print(\"Full rewards: \", rewards)\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_fn = knights_archers_zombies_v10\n",
    "\n",
    "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
    "\n",
    "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
    "    train(env_fn, steps=81_920, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
    "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
    "\n",
    "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
    "    # eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1e0da6-b085-4ea8-a06c-ab13af835a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation on knights_archers_zombies_v10 (num_games=1, render_mode=human)\n",
      "Avg reward: 0.5\n",
      "Avg reward per agent, per game:  {'archer_0': 1.0, 'archer_1': 1.0, 'knight_0': 0.0, 'knight_1': 0.0}\n",
      "Full rewards:  {'archer_0': 1, 'archer_1': 1, 'knight_0': 0, 'knight_1': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(env_fn, num_games=1, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef343e4-5282-486f-8c32-2b1bab4baf99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL",
   "language": "python",
   "name": "marl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
