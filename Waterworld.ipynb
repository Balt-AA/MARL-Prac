{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36456674-b350-4178-a435-fd23e30266ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca2288d-cd1e-44cb-b980-cb402f524533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on waterworld_v4.\n",
      "Using cpu device\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1401  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1090        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004160896 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.000374    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 4.47        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000227   |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1019         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055045765 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.293        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 7.36         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000735    |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 12.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 976        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00817028 |\n",
      "|    clip_fraction        | 0.0723     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.73      |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 7.67       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00167   |\n",
      "|    std                  | 0.943      |\n",
      "|    value_loss           | 15.6       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 953       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 171       |\n",
      "|    total_timesteps      | 163840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0059617 |\n",
      "|    clip_fraction        | 0.0699    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.7      |\n",
      "|    explained_variance   | 0.391     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 9.21      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.00179  |\n",
      "|    std                  | 0.927     |\n",
      "|    value_loss           | 17.7      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 940        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 209        |\n",
      "|    total_timesteps      | 196608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00661059 |\n",
      "|    clip_fraction        | 0.0922     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.66      |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 6.65       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0015    |\n",
      "|    std                  | 0.911      |\n",
      "|    value_loss           | 17.4       |\n",
      "----------------------------------------\n",
      "Model has been saved.\n",
      "Finished training on waterworld_v4.\n",
      "\n",
      "Starting evaluation on waterworld_v4 (num_games=10, render_mode=None)\n",
      "Rewards:  {'pursuer_0': 29.964124201094684, 'pursuer_1': -199.94022749228873}\n",
      "Avg reward: -84.98805164559703\n",
      "\n",
      "Starting evaluation on waterworld_v4 (num_games=2, render_mode=human)\n",
      "Rewards:  {'pursuer_0': 69.40262150857151, 'pursuer_1': -17.079676393446732}\n",
      "Avg reward: 26.16147255756239\n"
     ]
    }
   ],
   "source": [
    "def train_butterfly_supersuit(\n",
    "    env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = env_fn.parallel_env(**env_kwargs)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Note: Waterworld's observation space is discrete (242,) so we use an MLP policy rather than CNN\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: We train using the Parallel API but evaluate using the AEC API\n",
    "    # SB3 models are designed for single-agent settings, we get around this by using he same model for every agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    print(\"Rewards: \", rewards)\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_fn = waterworld_v4\n",
    "    env_kwargs = {}\n",
    "\n",
    "    # Train a model (takes ~3 minutes on GPU)\n",
    "    train_butterfly_supersuit(env_fn, steps=196_608, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 10 games (average reward should be positive but can vary significantly)\n",
    "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
    "\n",
    "    # Watch 2 games\n",
    "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b06b02-3622-44ec-bb5a-276e4cd20de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation on waterworld_v4 (num_games=1, render_mode=human)\n",
      "Rewards:  {'pursuer_0': 40.94844302524716, 'pursuer_1': -14.966827780865003}\n",
      "Avg reward: 12.990807622191078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.990807622191078"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(env_fn, num_games=1, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf48dc1-281c-4f8a-9d71-89f8e94ec7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL",
   "language": "python",
   "name": "marl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
